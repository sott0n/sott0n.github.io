<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SE Can&#39;t Code</title>
    <link>https://sott0n.github.io/</link>
    <description>Recent content on SE Can&#39;t Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Wed, 01 May 2019 22:06:41 +0900</lastBuildDate><atom:link href="https://sott0n.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>x86_64: レジスタについて</title>
      <link>https://sott0n.github.io/posts/2019/05/x86_64-%E3%83%AC%E3%82%B8%E3%82%B9%E3%82%BF%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Wed, 01 May 2019 22:06:41 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/05/x86_64-%E3%83%AC%E3%82%B8%E3%82%B9%E3%82%BF%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>CPUにはデータや実行している命令のアドレスを保存するためのレジスタという少量のメモリ領域がある。コンピュータで演算を行う際にはレジスタ上に保存されているデータやアドレスを元にして実行を行うようになっており、コンピュータ上ではもっともアクセス速度が速いデータ保存領域と言える。ここではx86_64に対してgdbを動かしながらレジスタについてまとめていこうと思う。
レジスタの種類 一言でレジスタと言ってもいろいろな用途があり、大きく下記のように一般的なCPUでは分類することができる。
 汎用レジスタ: データの保存用。x86_64では64bit値を保持できる。 プログラムカウンタ: 現在実行している命令のアドレスを保持する。この値を操作するのに特殊な命令が必要になる。 フラグレジスタ: 演算結果のうち、データではない結果を保持する。例えば、足し算の結果が64bit値に収まらない場合にそれを示す値が入れられる。 浮動小数レジスタ: 浮動小数演算のデータを保持する。  x86_64でgdbのinfo_registersを入力すると多くのレジスタが表示される。
   レジスタ名 内容     rax 汎用レジスタ   rbx 汎用レジスタ   rcx 汎用レジスタ   rdx 汎用レジスタ   rsi 汎用レジスタ   rdi 汎用レジスタ   rbp 汎用レジスタ   rsp 汎用レジスタ兼スタックポインタ   r8 汎用レジスタ   r9 汎用レジスタ   r10 汎用レジスタ   r11 汎用レジスタ   r12 汎用レジスタ   r13 汎用レジスタ   r14 汎用レジスタ   r15 汎用レジスタ   rip プログラムカウンタ   eflags フラグレジスタ   cs セグメントレジスタ   ss セグメントレジスタ   ds セグメントレジスタ   es セグメントレジスタ   fs セグメントレジスタ   gs セグメントレジスタ    このようにレジスタにも沢山あって、自分たちが気軽に書いているプログラムは目に見えないところでそれぞれのレジスタを利用してデータ操作をしながら命令を実行している。一つ一つのレジスタの容量は小さく、汎用レジスタは16個ほどあるが、それぞれ8byteの容量となるため全部で128byteという小さいサイズのデータしか保存することができない。当然、現代で使われるデータ(例えば画像とか)が128byteに収まるなんてあり得ないため、大きなデータを保存するためにコンピュータにはメモリという主記憶装置が存在し、また、SSDやHDDといった大容量データ保存用の場所が別で用意されている。</description>
    </item>
    
    <item>
      <title>MLIR: 新しいTFコンパイラインフラとなる中間表現について</title>
      <link>https://sott0n.github.io/posts/2019/04/mlir-%E6%96%B0%E3%81%97%E3%81%84tf%E3%82%B3%E3%83%B3%E3%83%91%E3%82%A4%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9%E3%81%A8%E3%81%AA%E3%82%8B%E4%B8%AD%E9%96%93%E8%A1%A8%E7%8F%BE%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Fri, 19 Apr 2019 07:50:26 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/04/mlir-%E6%96%B0%E3%81%97%E3%81%84tf%E3%82%B3%E3%83%B3%E3%83%91%E3%82%A4%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9%E3%81%A8%E3%81%AA%E3%82%8B%E4%B8%AD%E9%96%93%E8%A1%A8%E7%8F%BE%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>2019 European LLVM Developers Meetingを眺めていたらGoogleがMulti-Level Intermediate Representation for Compiler Infrastructureと言う 新しい中間表現の発表をしていたので気になって調べてみた。どうも複雑怪奇になっているTensorFlowのコンパイラエコシステムを一つにまとめる新しいインフラとしての中間表現っぽい。 いくつかの関連する資料を読んだのでそれぞれポイントを抜粋しようと思う。
tl;dr MLIR(Multi-Level Intermediate Representation)を一言で表すと:
 TensorFlowの新しいSSAベースの中間表現 複雑化しているTensorFlowのコンパイラ郡を統一する目的がある LLVMのデザインに強く影響を受けており、柔軟なインフラ設計となっている  位置付けとしては、一つ上位レイヤを追加して共通フォーマット的なメタ表現を設計したというイメージ。 これによって各アーキテクチャ毎に独自のIRを生成する必要がなくなって、開発者にとってはMLIRを意識すれば柔軟に開発が行えるというものだと思う。
複雑化したTFコンパイラ TensorFlowにはいくつかのコンパイラが存在する。それぞれCPU/GPU/TPU、モバイルといったターゲットとなるアーキテクチャ毎にコンパイラ(XLA/TensorRT/nGraph/CoreML/TensorFlow Lite etc)が存在し、 これらは開発が進むと共に複雑化していった。
たとえば、TensorFlow XLAの場合だと HLOと言う中間表現に変換してグラフ最適化などを実行した上で、LLVM IRに変換してCPUやGPU向けにコードジェネレーションを行ったり、 TPU IRに変換してTPU用に最適化をかけたりする。Tensor RTはNVIDIAが出しているコンパイラだが、NVIDIAのGPUアーキテクチャをターゲットにする場合はこれを利用するとNVIDIA特化の最適化が行われる。 モバイル上でDeep Learningを動かしたい場合はTensorFlow Liteを利用するだろう。
このような複雑怪奇な状態は開発者をとても困惑させる。と言うのも、それぞれのコンパイラではそれぞれの中間表現を設計しており、それらはとても似ているが決して同じものではないので、 複数のアーキテクチャをターゲットにした場合は一つ一つ最適化を掛け直し、path変換を行わなければならない。また、それぞれでエラーメッセージの設計も違うので、開発者をとても困惑させるだろう。
MLIRとは MLIRはこれらの問題を解決する新しい中間表現として設計されている。最新の最適化コンパイラのための柔軟なインフラで、また、LLVMのデザインに強く影響を受けている。 たとえば以下のようなところはLLVMと似たデザインとなっている:
 SSAベースな中間表現 柔軟な型システム tree address 構造が同じ(Module/Function/Block/Instruction) 同じ単位の中で複数レベルの抽象化を組み合わせたグラフの表現/分析/変換が可能  また、これらの抽象概念には、TensorFlow操作、入れ子になった多面体ループ領域、さらにはLLVM命令、さらには固定ハードウェア操作とタイプが含まれる。
先にも書いたがTensorFlowにはいくつかのコンパイラとそれに紐づく中間表現があり、 これらは演算子のセマンティクスが異なるため、言語の各層ごとに構築する表現が異なっている。 MLIRはそれらの中間表現に対するメタ表現的な位置付けで、共通フォーマット化をしようとするインフラとしての方針を指している。
MLIR Dialects MLIRは下記をターゲットとしてサポートしている:
 TensorFlow IR, which represents all things possible in TensorFlow graphs XLA HLO IR, which is designed to take advantage of XLA’s compilation abilities (with output to, among other things, TPUs) An experimental affine dialect, which focuses on polyhedral representations and optimizations LLVM IR, which has a 1:1 mapping between it and LLVM’s own representation, allowing MLIR to emit GPU and CPU code through LLVM TensorFlow Lite, which will translate to running code on mobile platforms  Dialectsはカスタム型を定義することができる。 これにより将来的にどのDialectsであれ、LLVM IR型システム(ファーストクラスの集合体を持つ)、量子化型のようなML最適化アクセラレータにとって重要なドメイン抽象化、そしてSwiftまたはClang型システムが得られる。</description>
    </item>
    
    <item>
      <title>ErgoDoxEZキーボードを導入する</title>
      <link>https://sott0n.github.io/posts/2019/04/ergodoxez%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%92%E5%B0%8E%E5%85%A5%E3%81%99%E3%82%8B/</link>
      <pubDate>Sun, 14 Apr 2019 01:50:17 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/04/ergodoxez%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%92%E5%B0%8E%E5%85%A5%E3%81%99%E3%82%8B/</guid>
      <description>分割エルゴノミクスキーボードで有名なErgoDox EZを導入したのでその設定方法についてメモを残そうと思う。 画像のものは自宅だが、自分はこのキーボードを職場用として使っている。端的に言うと、ErgoDoxサイコーなのでプログラマーはぜひ導入しましょう。
tl;dr ErgoDox EZの感想を述べると、
 ErgoDox EZの設定自体はそんなに難しくない ただしJISキーボードを想定する場合は結構辛い 自分好みにキーバインドが設定できるのでかなり楽しい  慣れるまで大変だけれど、3日もすれば余裕で慣れるし、そのあとはプログラミングがかなり楽になるのでオススメ。
ErgoDox EZとは ErgoDoxはキーボードの設計とソースコードをオープンにしているプロジェクトで、ErgoDox EZはそれを製品として販売しているものになる。
https://www.indiegogo.com/projects/ergodox-ez-an-incredible-mechanical-keyboard#/
自由にキーバインドをカスタマイズできたり、キー自体を色々自分で変えて遊んでみたりとプログラマーがオリジナルで自分のキーボードを作れて楽しめるような製品になっている。 購入すると海の向こう側から配送してもらうことになるのだが、1~2週間程度で日本の自宅までやってきた記憶がある。
キーバインド条件 ErgoDox EZが手元に届いたらまずやることはキーバインドの設定だろう。本エントリではこのキーバインドの設定についてのみメモを残していく。
自分の場合は下記の条件でキーバインドを設定した:
 WindowsとMacbookの両方で利用できること JISキーボードであること  自分がなぜ日本人なのだろうかと思うタイミングの一つにJISキーボード縛りがあると思う。 自分の場合、会社の特殊な環境のおかげでUSキーボードが何故か機能しないという現象に合い、仕方なくJISキーボードで設定をする羽目になった。 当然ながらこの手の海外産の製品はUSキーボードが前提になってしまうため、キーバインドも微妙な違いが出てくる。
また、職場がWindowsとMacbookの両方で作業しているので、両方で使えるようにキーバインドするのが結構大変だった。 ErgoDox EZはキーバインド自体をLayerとして複数持つことができ、かつそれらを自由に追加できるので、 Windows用のキーバインドとMacbook用のキーバインドを用意してあげたりもできる。自分は最終的にLayerを一つ追加してOS毎に対応するようにした。
キーバインドの設定方法 ErgoDox EZはTeensy USB Development Board と言うソフトウェアを使い、 コンパイルしたファームウェアをキーボードに流し込んでキーバインドを自由に設定することができる。
やり方はシンプルで下記のやり方で設定できる:
 Teensyをダウンロード ファームウェアをコンパイルして.hexを生成 Teensyに生成した.hexを流し込む  ここではまず、ファームウェアのコンパイル方法から説明する。
ファームウェアのコンパイル まずコンパイルに一通り必要なソフトウェアをインストールする。(MacOS前提)
brew tap osx-cross/avr brew install avr-libc brew install dfu-programmer そのあとは、https://github.com/qmk/qmk_firmware/tree/master/keyboards/ergodox_ezのソースコードをダウンロードし、 keymaps/defaultというディレクトリと同じ場所に自分自身のディレクトリを作成して、default以下のkeymap.cと同様のファイルを作成する（コピーして編集するといい）。 それをmake KEYMAP=[your_name]でmakeするとコンパイルされたhexファイルが作成される。hexファイルが作成されたらTeensyに読み込ませてキーボードに流し込むとキーバインドが反映される。
自分でkeymap.cを編集するのが一番自由度が高くて良いが、一番手っ取り早いやり方にGUI上でキーバインドを設定してhexファイルを生成する方法がある。
https://configure.ergodox-ez.com/layouts/default/latest/0
このサイトではGUIベースでキーバインドを決めれるのでとても楽に直感で設定ができる。また、Print layoutから印刷もできるのでキーバインドを覚えるように印刷するのがオススメ。
JISキーボード設定 さて、ここから自分が一番苦労したところである、JISキーボードの設定を書く。 書くといっても答えはシンプルで、USキーボードとは割当てられているキーがJISとは違うので変換して登録をしてあげるだけである。 たとえば、JISの場合の@はUSの場合だと[と打刻される。このように微妙に配列が違う箇所があるので、hexを生成する際にこの差分を吸収してあげないといけない。</description>
    </item>
    
    <item>
      <title>ONNXのdocを読む: ONNX versioning</title>
      <link>https://sott0n.github.io/posts/2019/03/onnx%E3%81%AEdoc%E3%82%92%E8%AA%AD%E3%82%80-onnx-versioning/</link>
      <pubDate>Wed, 06 Mar 2019 09:44:12 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/03/onnx%E3%81%AEdoc%E3%82%92%E8%AA%AD%E3%82%80-onnx-versioning/</guid>
      <description>ONNXのドキュメントをちゃんと読もうということで読んだところからメモとしていろいろまとめていこうと思う（今更だけど）。今回はONNX利用者がまず意識しないといけないversioningのドキュメントを読む。
Ref: https://github.com/onnx/onnx/blob/v1.4.1/docs/Versioning.md
 ONNX versioningの基本方針 ONNXのversioningには大きく三つのポリシーがある:
 IR version: グラフと演算子の抽象モデル、それらのフォーマットのバージョン管理。 Operator version: ONNX graphから参照されるオペレータ仕様のバージョン管理。 Model version: 定義済み/学習済みモデルのバージョン管理。  IR versionとOperator versionは守らなければいけないバージョン管理であるが、Model versionに関しては任意である（推奨されているだけ）。ローカルで独自に利用する場合はユーザーの判断に任されるが、オープンにシェアする前提のモデルに関してはversioninigをすべきである。
IR versioning 型定義などIRを構築するもののバージョン管理。ONNX IR formatはprotobuf3のUpdating a Message Typeに規定されたバージョン管理ガイドラインを準拠している。ONNXにはONNX checkerというものがあり、このツールによってルールが守られているかどうかがチェックされている。
Ref: Protobuf3 Updating a Message Type
Operator versioning Deep Learningの各命令（convやfcなど）のバージョン管理。operatorは3要素のtupleで定義されている。
 (domain.op_type:op_version) グラフ内のノードはいつもこの3要素を参照してoperatorを見るようになっている。また、破壊的なoperatorの変更は下記の通り:
  operator自体のadding/removing/renaming。また、新しいオプション属性の追加も含まれる operatorのinputとoutputのadding/removing/reordering。 型の追加と削除、あるいは変更 既存parameterのシグネチャの新しい振る舞い  ONNXモデルは二つのオペレータ情報（domain, opset_verison）を含んだリストを宣言する必要がある。空の文字列（&amp;quot;&amp;quot;）ドメインはONNX仕様の一部として定義されているオペレータであることを示す。与えられたモデルよって指定されたオペレータセットの和集合は、モデルグラフ内の各ノードに対して互換性のあるオペレータを宣言しなければならない。
Model versioning Model versioningはONNX変換して配布するユーザーによって依存するため、あくまでもONNXが推奨するプラクティスを提示しているだけである。なので、学習済みモデルをONNXのフォーマットに変換して配布する場合はONNXが推奨するバージョンの付け方に従うといい。推奨されるやり方としては破壊的変更であるケースと非破壊的変更のケースの二つに分かれる。 破壊的変更である場合はModelProto.model_versionのメジャーバージョンをインクリメントする。破壊的変更は下記のようなものを含む:
 カラー画像のインプットからグレースケールに変換するといったインプットまたはアウトプットの意味を変えてしまう変更 互換性のない型の変更 GraphProto内の値と同じ名前の値いったインプットの追加 既存アウトプットの削除  また、非破壊的変更はModelProto.model_versionのマイナーバージョンをインクリメントする。非破壊的変更は下記のようなものを含む: 互換性のある型の変更 意味を持った、または指定されたデフォルト値がある新しい入力の追加 以前のバージョンのグラフでは不可能だった入力によって引き起こされる新しい振る舞いの追加
 ONNXの躓くところとして、opsetのバージョンの差異があると思う。バージョンが違った際にはONNXが提供しているVersion Converterを利用することができるが、まだ上手く動かないといったことは多い。ONNXを利用する場合は、インプットとアウトプットのONNX対応のバージョンの互換性を確認する必要がある。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://sott0n.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sott0n.github.io/about/</guid>
      <description>About 👾 Who?
 Kohei Yamaguchi, software research engineer at Tokyo. GitHub: https://github.com/sott0n Twitter: https://twitter.com/sott0n  👾 Interests?
 System Programming, Compiler/OS/Filesystem/Emulator. WebAssembly x Machine Learning. MLOps, especially Kubeflow.  👾 Career?
 System Engineer : 2 years Data Scieitist : 5 years ML Researcher : 1 years~n  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://sott0n.github.io/host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sott0n.github.io/host/</guid>
      <description>Hosting static page  todo.react  This is a toy TODO app written React.    </description>
    </item>
    
  </channel>
</rss>
