<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ml on SE Can&#39;t Code</title>
    <link>https://sott0n.github.io/tags/ml/</link>
    <description>Recent content in ml on SE Can&#39;t Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 19 Apr 2019 07:50:26 +0900</lastBuildDate><atom:link href="https://sott0n.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MLIR: 新しいTFコンパイラインフラとなる中間表現について</title>
      <link>https://sott0n.github.io/posts/2019/04/mlir-%E6%96%B0%E3%81%97%E3%81%84tf%E3%82%B3%E3%83%B3%E3%83%91%E3%82%A4%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9%E3%81%A8%E3%81%AA%E3%82%8B%E4%B8%AD%E9%96%93%E8%A1%A8%E7%8F%BE%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Fri, 19 Apr 2019 07:50:26 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/04/mlir-%E6%96%B0%E3%81%97%E3%81%84tf%E3%82%B3%E3%83%B3%E3%83%91%E3%82%A4%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9%E3%81%A8%E3%81%AA%E3%82%8B%E4%B8%AD%E9%96%93%E8%A1%A8%E7%8F%BE%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>2019 European LLVM Developers Meetingを眺めていたらGoogleがMulti-Level Intermediate Representation for Compiler Infrastructureと言う 新しい中間表現の発表をしていたので気になって調べてみた。どうも複雑怪奇になっているTensorFlowのコンパイラエコシステムを一つにまとめる新しいインフラとしての中間表現っぽい。 いくつかの関連する資料を読んだのでそれぞれポイントを抜粋しようと思う。
tl;dr MLIR(Multi-Level Intermediate Representation)を一言で表すと:
 TensorFlowの新しいSSAベースの中間表現 複雑化しているTensorFlowのコンパイラ郡を統一する目的がある LLVMのデザインに強く影響を受けており、柔軟なインフラ設計となっている  位置付けとしては、一つ上位レイヤを追加して共通フォーマット的なメタ表現を設計したというイメージ。 これによって各アーキテクチャ毎に独自のIRを生成する必要がなくなって、開発者にとってはMLIRを意識すれば柔軟に開発が行えるというものだと思う。
複雑化したTFコンパイラ TensorFlowにはいくつかのコンパイラが存在する。それぞれCPU/GPU/TPU、モバイルといったターゲットとなるアーキテクチャ毎にコンパイラ(XLA/TensorRT/nGraph/CoreML/TensorFlow Lite etc)が存在し、 これらは開発が進むと共に複雑化していった。
たとえば、TensorFlow XLAの場合だと HLOと言う中間表現に変換してグラフ最適化などを実行した上で、LLVM IRに変換してCPUやGPU向けにコードジェネレーションを行ったり、 TPU IRに変換してTPU用に最適化をかけたりする。Tensor RTはNVIDIAが出しているコンパイラだが、NVIDIAのGPUアーキテクチャをターゲットにする場合はこれを利用するとNVIDIA特化の最適化が行われる。 モバイル上でDeep Learningを動かしたい場合はTensorFlow Liteを利用するだろう。
このような複雑怪奇な状態は開発者をとても困惑させる。と言うのも、それぞれのコンパイラではそれぞれの中間表現を設計しており、それらはとても似ているが決して同じものではないので、 複数のアーキテクチャをターゲットにした場合は一つ一つ最適化を掛け直し、path変換を行わなければならない。また、それぞれでエラーメッセージの設計も違うので、開発者をとても困惑させるだろう。
MLIRとは MLIRはこれらの問題を解決する新しい中間表現として設計されている。最新の最適化コンパイラのための柔軟なインフラで、また、LLVMのデザインに強く影響を受けている。 たとえば以下のようなところはLLVMと似たデザインとなっている:
 SSAベースな中間表現 柔軟な型システム tree address 構造が同じ(Module/Function/Block/Instruction) 同じ単位の中で複数レベルの抽象化を組み合わせたグラフの表現/分析/変換が可能  また、これらの抽象概念には、TensorFlow操作、入れ子になった多面体ループ領域、さらにはLLVM命令、さらには固定ハードウェア操作とタイプが含まれる。
先にも書いたがTensorFlowにはいくつかのコンパイラとそれに紐づく中間表現があり、 これらは演算子のセマンティクスが異なるため、言語の各層ごとに構築する表現が異なっている。 MLIRはそれらの中間表現に対するメタ表現的な位置付けで、共通フォーマット化をしようとするインフラとしての方針を指している。
MLIR Dialects MLIRは下記をターゲットとしてサポートしている:
 TensorFlow IR, which represents all things possible in TensorFlow graphs XLA HLO IR, which is designed to take advantage of XLA’s compilation abilities (with output to, among other things, TPUs) An experimental affine dialect, which focuses on polyhedral representations and optimizations LLVM IR, which has a 1:1 mapping between it and LLVM’s own representation, allowing MLIR to emit GPU and CPU code through LLVM TensorFlow Lite, which will translate to running code on mobile platforms  Dialectsはカスタム型を定義することができる。 これにより将来的にどのDialectsであれ、LLVM IR型システム(ファーストクラスの集合体を持つ)、量子化型のようなML最適化アクセラレータにとって重要なドメイン抽象化、そしてSwiftまたはClang型システムが得られる。</description>
    </item>
    
    <item>
      <title>ONNXのdocを読む: ONNX versioning</title>
      <link>https://sott0n.github.io/posts/2019/03/onnx%E3%81%AEdoc%E3%82%92%E8%AA%AD%E3%82%80-onnx-versioning/</link>
      <pubDate>Wed, 06 Mar 2019 09:44:12 +0900</pubDate>
      
      <guid>https://sott0n.github.io/posts/2019/03/onnx%E3%81%AEdoc%E3%82%92%E8%AA%AD%E3%82%80-onnx-versioning/</guid>
      <description>ONNXのドキュメントをちゃんと読もうということで読んだところからメモとしていろいろまとめていこうと思う（今更だけど）。今回はONNX利用者がまず意識しないといけないversioningのドキュメントを読む。
Ref: https://github.com/onnx/onnx/blob/v1.4.1/docs/Versioning.md
 ONNX versioningの基本方針 ONNXのversioningには大きく三つのポリシーがある:
 IR version: グラフと演算子の抽象モデル、それらのフォーマットのバージョン管理。 Operator version: ONNX graphから参照されるオペレータ仕様のバージョン管理。 Model version: 定義済み/学習済みモデルのバージョン管理。  IR versionとOperator versionは守らなければいけないバージョン管理であるが、Model versionに関しては任意である（推奨されているだけ）。ローカルで独自に利用する場合はユーザーの判断に任されるが、オープンにシェアする前提のモデルに関してはversioninigをすべきである。
IR versioning 型定義などIRを構築するもののバージョン管理。ONNX IR formatはprotobuf3のUpdating a Message Typeに規定されたバージョン管理ガイドラインを準拠している。ONNXにはONNX checkerというものがあり、このツールによってルールが守られているかどうかがチェックされている。
Ref: Protobuf3 Updating a Message Type
Operator versioning Deep Learningの各命令（convやfcなど）のバージョン管理。operatorは3要素のtupleで定義されている。
 (domain.op_type:op_version) グラフ内のノードはいつもこの3要素を参照してoperatorを見るようになっている。また、破壊的なoperatorの変更は下記の通り:
  operator自体のadding/removing/renaming。また、新しいオプション属性の追加も含まれる operatorのinputとoutputのadding/removing/reordering。 型の追加と削除、あるいは変更 既存parameterのシグネチャの新しい振る舞い  ONNXモデルは二つのオペレータ情報（domain, opset_verison）を含んだリストを宣言する必要がある。空の文字列（&amp;quot;&amp;quot;）ドメインはONNX仕様の一部として定義されているオペレータであることを示す。与えられたモデルよって指定されたオペレータセットの和集合は、モデルグラフ内の各ノードに対して互換性のあるオペレータを宣言しなければならない。
Model versioning Model versioningはONNX変換して配布するユーザーによって依存するため、あくまでもONNXが推奨するプラクティスを提示しているだけである。なので、学習済みモデルをONNXのフォーマットに変換して配布する場合はONNXが推奨するバージョンの付け方に従うといい。推奨されるやり方としては破壊的変更であるケースと非破壊的変更のケースの二つに分かれる。 破壊的変更である場合はModelProto.model_versionのメジャーバージョンをインクリメントする。破壊的変更は下記のようなものを含む:
 カラー画像のインプットからグレースケールに変換するといったインプットまたはアウトプットの意味を変えてしまう変更 互換性のない型の変更 GraphProto内の値と同じ名前の値いったインプットの追加 既存アウトプットの削除  また、非破壊的変更はModelProto.model_versionのマイナーバージョンをインクリメントする。非破壊的変更は下記のようなものを含む: 互換性のある型の変更 意味を持った、または指定されたデフォルト値がある新しい入力の追加 以前のバージョンのグラフでは不可能だった入力によって引き起こされる新しい振る舞いの追加
 ONNXの躓くところとして、opsetのバージョンの差異があると思う。バージョンが違った際にはONNXが提供しているVersion Converterを利用することができるが、まだ上手く動かないといったことは多い。ONNXを利用する場合は、インプットとアウトプットのONNX対応のバージョンの互換性を確認する必要がある。</description>
    </item>
    
  </channel>
</rss>
