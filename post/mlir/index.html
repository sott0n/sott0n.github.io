<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<title> MLIR: 新しいTFコンパイラインフラとなる中間表現について &middot; SE Can&#39;t Code </title>


<link rel="stylesheet" href="https://sott0n.github.io/css/slim.css">
<link rel="stylesheet" href="https://sott0n.github.io/css/highlight.min.css">
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Source+Code+Pro' rel='stylesheet' type='text/css'>

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="/favicon.ico">


<link href="" rel="alternate" type="application/rss+xml" title="SE Can&#39;t Code" />

</head>

<body>
  <div class="container">
    <div class="header">
  <h1 class="site-title"><a href="https://sott0n.github.io/">SE Can&#39;t Code</a></h1>
  <p class="site-tagline">I&#39;m Software Engineer, not System Engineer.</p>
  <div class="nav">
    <a class="nav-btn" href="#">
      <span class="ci ci-burger"></span>
    </a>
    <ul class="nav-list">
       
	  <li class="spacer">&ac;</li>

      <li><a href="https://github.com/sott0n">Github</a></li> 
      <li><a href="https://twitter.com/sott0n_">Twitter</a></li> 
    </ul>
  </div>
</div>
    <div class="content">
      <div class="posts">
        <div class="post">
          <h2 class="post-title"><a href="https://sott0n.github.io/post/mlir/">MLIR: 新しいTFコンパイラインフラとなる中間表現について</a></h2>
          <span class="post-date">Apr 19, 2019 </span>
          <div class="post-content">
            <p><code>2019 European LLVM Developers Meeting</code>を眺めていたらGoogleが<code>Multi-Level Intermediate Representation for Compiler Infrastructure</code>と言う
新しい中間表現の発表をしていたので気になって調べてみた。どうも複雑怪奇になっているTensorFlowのコンパイラエコシステムを一つにまとめる新しいインフラとしての中間表現っぽい。
いくつかの関連する資料を読んだのでそれぞれポイントを抜粋しようと思う。</p>
<h2 id="tldr">tl;dr</h2>
<p>MLIR(Multi-Level Intermediate Representation)を一言で表すと:</p>
<ul>
<li>TensorFlowの新しいSSAベースの中間表現</li>
<li>複雑化しているTensorFlowのコンパイラ郡を統一する目的がある</li>
<li>LLVMのデザインに強く影響を受けており、柔軟なインフラ設計となっている</li>
</ul>
<p>位置付けとしては、一つ上位レイヤを追加して共通フォーマット的なメタ表現を設計したというイメージ。
これによって各アーキテクチャ毎に独自のIRを生成する必要がなくなって、開発者にとってはMLIRを意識すれば柔軟に開発が行えるというものだと思う。</p>
<h2 id="複雑化したtfコンパイラ">複雑化したTFコンパイラ</h2>
<p>TensorFlowにはいくつかのコンパイラが存在する。それぞれCPU/GPU/TPU、モバイルといったターゲットとなるアーキテクチャ毎にコンパイラ(XLA/TensorRT/nGraph/CoreML/TensorFlow Lite etc)が存在し、
これらは開発が進むと共に複雑化していった。</p>
<p><img src="/image/mlir_image.png" alt="image" title="ref: https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d"></p>
<p>たとえば、TensorFlow XLAの場合だと HLOと言う中間表現に変換してグラフ最適化などを実行した上で、LLVM IRに変換してCPUやGPU向けにコードジェネレーションを行ったり、
TPU IRに変換してTPU用に最適化をかけたりする。Tensor RTはNVIDIAが出しているコンパイラだが、NVIDIAのGPUアーキテクチャをターゲットにする場合はこれを利用するとNVIDIA特化の最適化が行われる。
モバイル上でDeep Learningを動かしたい場合はTensorFlow Liteを利用するだろう。</p>
<p>このような複雑怪奇な状態は開発者をとても困惑させる。と言うのも、それぞれのコンパイラではそれぞれの中間表現を設計しており、それらはとても似ているが決して同じものではないので、
複数のアーキテクチャをターゲットにした場合は一つ一つ最適化を掛け直し、path変換を行わなければならない。また、それぞれでエラーメッセージの設計も違うので、開発者をとても困惑させるだろう。</p>
<h2 id="mlirとは">MLIRとは</h2>
<p>MLIRはこれらの問題を解決する新しい中間表現として設計されている。最新の最適化コンパイラのための柔軟なインフラで、また、LLVMのデザインに強く影響を受けている。
たとえば以下のようなところはLLVMと似たデザインとなっている:</p>
<ul>
<li>SSAベースな中間表現</li>
<li>柔軟な型システム</li>
<li>tree address</li>
<li>構造が同じ(Module/Function/Block/Instruction)</li>
<li>同じ単位の中で複数レベルの抽象化を組み合わせたグラフの表現/分析/変換が可能</li>
</ul>
<p>また、これらの抽象概念には、TensorFlow操作、入れ子になった多面体ループ領域、さらにはLLVM命令、さらには固定ハードウェア操作とタイプが含まれる。</p>
<p>先にも書いたがTensorFlowにはいくつかのコンパイラとそれに紐づく中間表現があり、
これらは演算子のセマンティクスが異なるため、言語の各層ごとに構築する表現が異なっている。
MLIRはそれらの中間表現に対するメタ表現的な位置付けで、共通フォーマット化をしようとするインフラとしての方針を指している。</p>
<h2 id="mlir-dialects">MLIR Dialects</h2>
<p>MLIRは下記をターゲットとしてサポートしている:</p>
<ul>
<li>TensorFlow IR, which represents all things possible in TensorFlow graphs</li>
<li>XLA HLO IR, which is designed to take advantage of XLA’s compilation abilities (with output to, among other things, TPUs)</li>
<li>An experimental affine dialect, which focuses on polyhedral representations and optimizations</li>
<li>LLVM IR, which has a 1:1 mapping between it and LLVM’s own representation, allowing MLIR to emit GPU and CPU code through LLVM</li>
<li>TensorFlow Lite, which will translate to running code on mobile platforms</li>
</ul>
<p><code>Dialects</code>はカスタム型を定義することができる。
これにより将来的にどの<code>Dialects</code>であれ、LLVM IR型システム(ファーストクラスの集合体を持つ)、量子化型のようなML最適化アクセラレータにとって重要なドメイン抽象化、そしてSwiftまたはClang型システムが得られる。</p>
<p>新しい低レベルコンパイラを接続したい場合は、新しい<code>Dialects</code>とTensorFlow Graph dialectsと自身の<code>Dialects</code>の間の下位部分を作成する。
これにより同じモデルの異なるレベルで<code>Dialects</code>をターゲットにすることさえできる。
また、MLIRはあらゆるレベルでの変換を構成することを可能にし、IRで自分自身の操作と抽象化を定義することができる。</p>
<hr>
<h3 id="reference">Reference</h3>
<ul>
<li><a href="https://github.com/tensorflow/mlir">MLIR</a></li>
<li><a href="https://drive.google.com/file/d/1hUeAJXcAXwz82RXA5VtO5ZoH8cVQhrOK/view">MLIR Primer:A Compiler Infrastructure for the End of Moore’s Law</a></li>
<li><a href="https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d">MLIR: A new intermediate representation and compiler framework</a></li>
</ul>

          </div>
        </div>
        <div class="pagination">
          <a class="btn previous " href="https://sott0n.github.io/post/ergodox/"> Prev</a>  
          <a class="btn next " href="https://sott0n.github.io/post/x86_64_basic/"> Next</a> 
        </div>
      </div>
    </div>
    
    <div class="footer">
  
  <p>Powered by <a href="https://gohugo.io">Hugo</a>. This theme—Slim—is open sourced on <a href="https://github.com/zhe/hugo-theme-slim">Github</a>.</p>
  
</div>

  </div>
  <script src="https://sott0n.github.io/js/slim.js"></script>
  <script src="https://sott0n.github.io/js/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
</body>

</html>
